services:
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    env_file: .env
    environment:
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_HOST:     http://ollama:11434
    ports:
      - "8000:8000"
    depends_on:
      - db
      - redis
      - minio
      - ollama
    volumes:
      - ./:/app

  worker:
    build:
      context: .
      dockerfile: Dockerfile.api
    command: ["celery", "-A", "backend.app.workers.celery_app:celery_app", "worker", "--loglevel=info"]
    env_file: .env
    environment:
      RUN_MIGRATIONS: "0"
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_HOST:     http://ollama:11434
    depends_on:
      - db
      - redis
      - minio
      - ollama
    volumes:
      - ./:/app

  ollama:
    image: ollama/ollama:latest
    # Host 11435 -> container 11434
    ports:
      - "${OLLAMA_HOST_PORT:-11435}:11434"
    # Let Ollama listen on its default container port/address
    environment:
      OLLAMA_HOST: 0.0.0.0:11434
    # If you have NVIDIA Container Toolkit installed, expose the GPU:
    gpus: all
    volumes:
      - ollama-data:/root/.ollama

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    ports:
      - "3000:3000"
    depends_on:
      - api
    environment:
      - VITE_API_URL=http://localhost:8000

  db:
    image: pgvector/pgvector:pg16
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: postgres
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - pgdata:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  minio:
    image: minio/minio:RELEASE.2024-08-03T04-33-23Z
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: ["server", "/data", "--console-address", ":9001"]
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio-data:/data

volumes:
  pgdata:
  minio-data:
  ollama-data:
